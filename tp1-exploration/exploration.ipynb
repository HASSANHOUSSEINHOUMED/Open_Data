{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d50618df-5ce0-4830-b83f-3a2ddde05ce5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cl√© Claude configur√©e avec succ√®s.\n",
      "Test de connexion √† Claude Sonnet 4.5...\n",
      "R√©ponse : OK\n",
      "Claude Sonnet 4.5 est op√©rationnel !\n"
     ]
    }
   ],
   "source": [
    "# Partie 0\n",
    "## Partie O.1 faite sur Powershell\n",
    "## Partie 0.2 faite sur Powershell\n",
    "# Partie 0.3 : Configuration de LiteLLM avec Claude API\n",
    "# Utilisation de Claude Sonnet 4.5, le dernier mod√®le disponible\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from litellm import completion\n",
    "\n",
    "# Chargement du fichier .env\n",
    "load_dotenv()\n",
    "\n",
    "# R√©cup√©ration de la cl√© API Claude\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "\n",
    "# V√©rification que la cl√© est bien charg√©e\n",
    "if anthropic_api_key:\n",
    "    print(\"Cl√© Claude configur√©e avec succ√®s.\")\n",
    "else:\n",
    "    print(\"Erreur : cl√© Claude non trouv√©e dans .env\")\n",
    "\n",
    "# Test rapide de connexion avec Claude Sonnet 4.5\n",
    "print(\"Test de connexion √† Claude Sonnet 4.5...\")\n",
    "\n",
    "response = completion(\n",
    "    model=\"claude-sonnet-4-5-20250929\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"R√©ponds juste 'OK'\"}],\n",
    "    api_key=anthropic_api_key\n",
    ")\n",
    "\n",
    "print(f\"R√©ponse : {response.choices[0].message.content}\")\n",
    "print(\"Claude Sonnet 4.5 est op√©rationnel !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c044dc44-b873-45fe-8cf4-5d00c36ac74e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V√©rification du setup compl√®te :\n",
      "--------------------------------------------------\n",
      "pandas: OK\n",
      "duckdb: OK\n",
      "plotly: OK\n",
      "litellm: OK\n",
      "--------------------------------------------------\n",
      "Setup termin√© ! Pr√™t pour l'exploration.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Partie 0.4 : V√©rification compl√®te du setup\n",
    "# Contr√¥le que tous les composants sont op√©rationnels\n",
    "\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "import plotly.express as px\n",
    "\n",
    "print(\"V√©rification du setup compl√®te :\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# V√©rifier que les modules importent correctement\n",
    "libraries = [\"pandas\", \"duckdb\", \"plotly\", \"litellm\"]\n",
    "for lib in libraries:\n",
    "    try:\n",
    "        __import__(lib)\n",
    "        print(f\"{lib}: OK\")\n",
    "    except ImportError:\n",
    "        print(f\"{lib}: ERREUR\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"Setup termin√© ! Pr√™t pour l'exploration.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "202b6bd5-a8a9-42a2-af41-f7caa7815fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du dataset accidents corporels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hassa\\AppData\\Local\\Temp\\ipykernel_12888\\545573002.py:10: DtypeWarning: Columns (15,29,31,33,34) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('accidents.csv', encoding='latin-1', sep=',', on_bad_lines='skip')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset charg√© avec succ√®s !\n",
      "Dimensions : 439332 lignes, 36 colonnes\n",
      "\n",
      "Premi√®res lignes du dataset :\n",
      "   org  lum  agg  int  atm  col  com  dep  catr  infra  ...  numero  \\\n",
      "0    5    2    6    1  8.0  3.0   53   10   4.0    0.0  ...     NaN   \n",
      "1    5    1    1    1  1.0  2.0   53   10   3.0    0.0  ...     NaN   \n",
      "2    3    3    1    1  5.0  5.0  248   10   1.0    0.0  ...     NaN   \n",
      "3    1    3    1    1  1.0  7.0  366   10   3.0    0.0  ...     NaN   \n",
      "4    1    1    5    1  1.0  2.0    4   10   4.0    5.0  ...     NaN   \n",
      "\n",
      "   distancemetre           libellevoie  coderivoli  grav  gps  lat  long  adr  \\\n",
      "0            0.0  23EME R.I. ( RUE DU)         NaN  0.43  NaN  NaN   NaN  NaN   \n",
      "1            NaN                   NaN         NaN  0.43  NaN  NaN   NaN  NaN   \n",
      "2            0.0                  A46N         NaN  0.43  NaN  NaN   NaN  NaN   \n",
      "3            NaN                   NaN         NaN  0.43  NaN  0.0   0.0  NaN   \n",
      "4            NaN                   NaN         NaN  0.43  NaN  0.0   0.0  NaN   \n",
      "\n",
      "   numac  \n",
      "0    1.0  \n",
      "1    2.0  \n",
      "2    3.0  \n",
      "3    4.0  \n",
      "4    5.0  \n",
      "\n",
      "[5 rows x 36 columns]\n",
      "\n",
      "Noms des colonnes :\n",
      "['org', 'lum', 'agg', 'int', 'atm', 'col', 'com', 'dep', 'catr', 'infra', 'voie', 'v1', 'v2', 'circ', 'nbv', 'pr', 'pr1', 'vosp', 'prof', 'plan', 'situ', 'ttue', 'tbg', 'tbl', 'tindm', 'typenumero', 'numero', 'distancemetre', 'libellevoie', 'coderivoli', 'grav', 'gps', 'lat', 'long', 'adr', 'numac']\n"
     ]
    }
   ],
   "source": [
    "# Partie 1.2 : Chargement du dataset\n",
    "# Chargement avec les bons param√®tres de s√©paration\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Chargement du dataset accidents corporels...\")\n",
    "\n",
    "# Chargement avec virgule comme s√©parateur et encodage latin-1\n",
    "# on_bad_lines='skip' : ignore les lignes mal format√©es s'il y en a\n",
    "df = pd.read_csv('accidents.csv', encoding='latin-1', sep=',', on_bad_lines='skip')\n",
    "\n",
    "print(f\"Dataset charg√© avec succ√®s !\")\n",
    "print(f\"Dimensions : {df.shape[0]} lignes, {df.shape[1]} colonnes\")\n",
    "print()\n",
    "\n",
    "# Aper√ßu des premi√®res lignes\n",
    "print(\"Premi√®res lignes du dataset :\")\n",
    "print(df.head())\n",
    "\n",
    "print()\n",
    "print(\"Noms des colonnes :\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b50208cb-3455-4cf3-9cef-db5a68bb1581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fonction d'assistance IA cr√©√©e avec succ√®s.\n"
     ]
    }
   ],
   "source": [
    "# Partie 2.1 : Cr√©ation de la fonction d'assistance IA\n",
    "# Fonction r√©utilisable pour poser des questions sur le dataset\n",
    "\n",
    "from litellm import completion\n",
    "import os\n",
    "\n",
    "def ask_data_assistant(question: str, context: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Fonction pour poser une question √† l'assistant IA sur les donn√©es\n",
    "    \n",
    "    Param√®tres :\n",
    "        question : la question √† poser sur le dataset\n",
    "        context : contexte du dataset (colonnes, statistiques, etc.)\n",
    "    \n",
    "    Retour :\n",
    "        La r√©ponse de l'assistant en tant que string\n",
    "    \"\"\"\n",
    "    \n",
    "    # Instruction syst√®me pour configurer le comportement de Claude\n",
    "    system_prompt = \"\"\"Tu es un expert en analyse de donn√©es.\n",
    "    Aide √† explorer et comprendre des datasets Open Data.\n",
    "    Sois concis et pratique dans les r√©ponses.\n",
    "    Propose du code Python avec pandas ou plotly quand c'est pertinent.\"\"\"\n",
    "    \n",
    "    # Construction du message pour Claude\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"{context}\\n\\nQuestion: {question}\"}\n",
    "    ]\n",
    "    \n",
    "    # Appel √† Claude Sonnet 4.5\n",
    "    response = completion(\n",
    "        model=\"claude-sonnet-4-5-20250929\",\n",
    "        messages=messages,\n",
    "        api_key=os.getenv('ANTHROPIC_API_KEY')\n",
    "    )\n",
    "    \n",
    "    # Retour de la r√©ponse\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print(\"Fonction d'assistance IA cr√©√©e avec succ√®s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4bc2ba2-c38c-4176-8b3a-929cc87dc1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hassa\\tp1-exploration\\.venv\\Lib\\site-packages\\pandas\\core\\nanops.py:1016: RuntimeWarning: overflow encountered in square\n",
      "  sqr = _ensure_numeric((avg - values) ** 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contexte du dataset g√©n√©r√© avec succ√®s.\n",
      "Extrait (premi√®res 500 caract√®res) :\n",
      "\n",
      "INFORMATIONS G√âN√âRALES :\n",
      "- Nombre de lignes : 439332\n",
      "- Nombre de colonnes : 36\n",
      "\n",
      "COLONNES ET TYPES :\n",
      "org                int64\n",
      "lum                int64\n",
      "agg                int64\n",
      "int                int64\n",
      "atm              float64\n",
      "col              float64\n",
      "com                int64\n",
      "dep                int64\n",
      "catr             float64\n",
      "infra            float64\n",
      "voie             float64\n",
      "v1               float64\n",
      "v2                object\n",
      "circ             float64\n",
      "nbv              float64\n",
      "pr                object\n"
     ]
    }
   ],
   "source": [
    "# Partie 2.2 : G√©n√©ration du contexte du dataset\n",
    "# Cr√©ation d'un r√©sum√© structur√© des donn√©es pour l'IA\n",
    "\n",
    "def generate_dataset_context(df) -> str:\n",
    "    \"\"\"\n",
    "    Fonction qui g√©n√®re un r√©sum√© complet du dataset pour l'IA\n",
    "    \n",
    "    Param√®tres :\n",
    "        df : le DataFrame pandas contenant les donn√©es\n",
    "    \n",
    "    Retour :\n",
    "        Un string avec toutes les informations du dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    # Cr√©ation du r√©sum√© structur√©\n",
    "    context = f\"\"\"\n",
    "INFORMATIONS G√âN√âRALES :\n",
    "- Nombre de lignes : {len(df)}\n",
    "- Nombre de colonnes : {len(df.columns)}\n",
    "\n",
    "COLONNES ET TYPES :\n",
    "{df.dtypes.to_string()}\n",
    "\n",
    "STATISTIQUES NUM√âRIQUES :\n",
    "{df.describe().to_string()}\n",
    "\n",
    "VALEURS MANQUANTES PAR COLONNE :\n",
    "{df.isnull().sum().to_string()}\n",
    "\n",
    "PREMI√àRES LIGNES :\n",
    "{df.head(3).to_string()}\n",
    "\"\"\"\n",
    "    return context\n",
    "\n",
    "# G√©n√©ration du contexte\n",
    "dataset_context = generate_dataset_context(df)\n",
    "\n",
    "# V√©rification\n",
    "print(\"Contexte du dataset g√©n√©r√© avec succ√®s.\")\n",
    "print(\"Extrait (premi√®res 500 caract√®res) :\")\n",
    "print(dataset_context[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "192ba557-b6f4-4349-ad02-da03faaeb4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "QUESTION 1 : Description du dataset\n",
      "======================================================================\n",
      "# Dataset d'Accidents de la Route en France üöó\n",
      "\n",
      "Ce dataset contient **439 332 accidents de la route** recens√©s en France, avec 36 variables d√©crivant les circonstances de chaque accident (luminosit√©, conditions atmosph√©riques, type de route, localisation, etc.).\n",
      "\n",
      "Les donn√©es couvrent la **gravit√© des accidents** (variable `grav`), le **nombre de victimes** (tu√©s, bless√©s graves/l√©gers via `ttue`, `tbg`, `tbl`), et les **caract√©ristiques g√©ographiques et routi√®res** (d√©partement, commune, cat√©gorie de route, intersection, etc.).\n",
      "\n",
      "Il s'agit manifestement de la **base BAAC (Bulletin d'Analyse des Accidents Corporels)** g√©r√©e par l'Observatoire National Interminist√©riel de la S√©curit√© Routi√®re, utilis√©e pour analyser la s√©curit√© routi√®re et identifier les facteurs d'accidents en France.\n",
      "\n",
      "**üìä Points cl√©s:**\n",
      "- Pr√®s de 280k accidents n'ont pas de coordonn√©es GPS compl√®tes\n",
      "- Moyenne de 0.06 tu√© et 1.25 bless√©s par accident\n",
      "- Donn√©es couvrant 97 d√©partements diff√©rents\n",
      "\n",
      "======================================================================\n",
      "QUESTION 2 : Probl√®mes de qualit√© de donn√©es\n",
      "======================================================================\n",
      "# 5 Principaux Probl√®mes de Qualit√© de Donn√©es\n",
      "\n",
      "## 1. **Taux de valeurs manquantes critiques** üö®\n",
      "- **`gps` : 98.9%** manquant (434,814/439,332)\n",
      "- **`adr` : 97.5%** manquant \n",
      "- **`lat`/`long` : ~64%** manquant\n",
      "- **`v2` : 95.1%** manquant\n",
      "\n",
      "Ces donn√©es de g√©olocalisation sont essentielles pour une base accidents, leur absence limite fortement les analyses spatiales.\n",
      "\n",
      "## 2. **Incoh√©rences dans les types de donn√©es**\n",
      "```python\n",
      "# Colonnes qui devraient √™tre num√©riques mais sont 'object'\n",
      "- v2 : object (devrait √™tre float/int)\n",
      "- pr : object (devrait √™tre num√©rique)\n",
      "- long : object (devrait √™tre float)\n",
      "- gps : object (format incertain)\n",
      "```\n",
      "\n",
      "## 3. **Valeurs aberrantes extr√™mes**\n",
      "```python\n",
      "# Exemples pr√©occupants :\n",
      "- numero : max = 8e+280 (!!!) vs m√©diane = 9\n",
      "- lat : max = 1.93e+80 vs valeurs normales France (42-51¬∞)\n",
      "- typenumero : std = inf (indication de valeurs infinies)\n",
      "- voie : max = 90000 (code voie anormal)\n",
      "```\n",
      "\n",
      "## 4. **Codification non document√©e**\n",
      "- Toutes les colonnes utilisent des codes num√©riques (1-9) sans documentation\n",
      "- Impossible de comprendre : `atm`, `col`, `lum`, `int`, `grav`, etc.\n",
      "- Les `0.0` repr√©sentent-ils des valeurs nulles ou une cat√©gorie ?\n",
      "\n",
      "## 5. **Incompl√©tude des enregistrements**\n",
      "- Seulement **42%** des lignes ont des informations de localisation pr√©cise (pr, pr1)\n",
      "- **30%** n'ont pas de num√©ro de voie\n",
      "- **28%** manquent de code Rivoli (r√©f√©rentiel voirie)\n",
      "\n",
      "### üîç Action recommand√©e :\n",
      "```python\n",
      "# Priorit√© 1 : Nettoyer les valeurs aberrantes\n",
      "df['numero'] = df['numero'].clip(upper=10000)\n",
      "df['lat'] = df['lat'].clip(lower=41, upper=52)\n",
      "\n",
      "# Priorit√© 2 : Convertir les types\n",
      "df['long'] = pd.to_numeric(df['long'], errors='coerce')\n",
      "```\n",
      "\n",
      "======================================================================\n",
      "QUESTION 3 : Proposer des analyses\n",
      "======================================================================\n",
      "# 3 Analyses int√©ressantes sur ces donn√©es d'accidents\n",
      "\n",
      "Voici trois analyses pertinentes bas√©es sur ce dataset d'accidents de la route :\n",
      "\n",
      "## 1. **Analyse temporelle et conditions de luminosit√©**\n",
      "√âtudier la relation entre la luminosit√© (`lum`) et la gravit√© des accidents (`grav`).\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import plotly.express as px\n",
      "\n",
      "# R√©partition gravit√© selon luminosit√©\n",
      "pivot = df.groupby(['lum', 'grav']).size().reset_index(name='count')\n",
      "fig = px.bar(pivot, x='lum', y='count', color='grav',\n",
      "             title='Gravit√© des accidents selon conditions de luminosit√©',\n",
      "             labels={'lum': 'Luminosit√© (1=Jour, 2=Cr√©puscule, 3-5=Nuit)', \n",
      "                     'grav': 'Gravit√©'})\n",
      "fig.show()\n",
      "\n",
      "# Taux de mortalit√© par condition\n",
      "df['mortel'] = df['ttue'] > 0\n",
      "taux_mortalite = df.groupby('lum')['mortel'].mean() * 100\n",
      "```\n",
      "\n",
      "**Int√©r√™t** : Identifier si la nuit est vraiment plus dangereuse et quantifier l'impact.\n",
      "\n",
      "## 2. **Cartographie des zones accidentog√®nes**\n",
      "Identifier les zones g√©ographiques √† risque (d√©partements, communes).\n",
      "\n",
      "```python\n",
      "# Top 10 d√©partements avec le plus d'accidents graves\n",
      "accidents_graves = df[df['grav'] >= 3]  # Bless√©s hospitalis√©s + tu√©s\n",
      "top_dep = accidents_graves['dep'].value_counts().head(10)\n",
      "\n",
      "# Analyse par type de route (catr) et d√©partement\n",
      "danger_route = df.groupby(['dep', 'catr']).agg({\n",
      "    'ttue': 'sum',\n",
      "    'grav': 'mean'\n",
      "}).sort_values('ttue', ascending=False)\n",
      "```\n",
      "\n",
      "**Int√©r√™t** : Prioriser les investissements en s√©curit√© routi√®re par r√©gion et type d'infrastructure.\n",
      "\n",
      "## 3. **Profil des accidents urbains vs ruraux**\n",
      "Comparer les caract√©ristiques des accidents en agglom√©ration (`agg`) vs hors agglom√©ration.\n",
      "\n",
      "```python\n",
      "# Comparaison multi-crit√®res\n",
      "comparaison = df.groupby('agg').agg({\n",
      "    'grav': 'mean',\n",
      "    'ttue': 'sum',\n",
      "    'tbg': 'sum',\n",
      "    'lum': lambda x: (x == 1).mean(),  # % accidents de jour\n",
      "    'atm': lambda x: (x == 1).mean(),   # % temps normal\n",
      "    'col': lambda x: x.value_counts(normalize=True)\n",
      "}).round(2)\n",
      "\n",
      "# Vitesse et gravit√©\n",
      "df_vitesse = df[df['v1'].notna()]\n",
      "fig = px.scatter(df_vitesse, x='v1', y='grav', color='agg',\n",
      "                 title='Relation vitesse/gravit√© selon type de zone',\n",
      "                 opacity=0.3)\n",
      "```\n",
      "\n",
      "**Int√©r√™t** : Adapter les politiques de pr√©vention (limitations de vitesse, signalisation) selon le contexte urbain/rural.\n",
      "\n",
      "---\n",
      "\n",
      "**Bonus rapide** : Analyse des collisions multiples (`nbv` > 2) et facteurs m√©t√©o (`atm`) pour cibler les messages de pr√©vention saisonniers.\n",
      "\n",
      "======================================================================\n",
      "QUESTION 4 : Code de nettoyage\n",
      "======================================================================\n",
      "# Analyse et nettoyage des valeurs manquantes\n",
      "\n",
      "Voici une approche structur√©e pour nettoyer ce dataset d'accidents :\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# Charger les donn√©es\n",
      "df = pd.read_csv('votre_fichier.csv')\n",
      "\n",
      "# ========================================\n",
      "# 1. ANALYSE DES VALEURS MANQUANTES\n",
      "# ========================================\n",
      "\n",
      "def analyser_valeurs_manquantes(df):\n",
      "    \"\"\"Analyse d√©taill√©e des valeurs manquantes\"\"\"\n",
      "    missing = pd.DataFrame({\n",
      "        'nb_manquants': df.isnull().sum(),\n",
      "        'pct_manquants': (df.isnull().sum() / len(df) * 100).round(2)\n",
      "    })\n",
      "    missing = missing[missing['nb_manquants'] > 0].sort_values('pct_manquants', ascending=False)\n",
      "    print(missing)\n",
      "    return missing\n",
      "\n",
      "analyser_valeurs_manquantes(df)\n",
      "\n",
      "# ========================================\n",
      "# 2. STRAT√âGIE DE NETTOYAGE\n",
      "# ========================================\n",
      "\n",
      "def nettoyer_donnees(df):\n",
      "    \"\"\"Nettoyage adapt√© par type de colonne\"\"\"\n",
      "    \n",
      "    df_clean = df.copy()\n",
      "    \n",
      "    # --- COLONNES √Ä SUPPRIMER (>90% manquantes) ---\n",
      "    colonnes_a_supprimer = ['gps', 'coderivoli', 'adr']\n",
      "    df_clean = df_clean.drop(columns=colonnes_a_supprimer)\n",
      "    print(f\"‚úì Suppression de {len(colonnes_a_supprimer)} colonnes tr√®s incompl√®tes\")\n",
      "    \n",
      "    # --- COLONNES G√âOGRAPHIQUES (lat/long) ---\n",
      "    # Supprimer les lignes sans coordonn√©es si analyse g√©ographique n√©cessaire\n",
      "    # OU garder toutes les lignes selon le besoin\n",
      "    # Option 1 : Garder tout (recommand√©)\n",
      "    df_clean['lat'].fillna(-999, inplace=True)  # Valeur sentinelle\n",
      "    df_clean['long'] = df_clean['long'].replace('', np.nan)\n",
      "    df_clean['long'].fillna('-999', inplace=True)\n",
      "    \n",
      "    # Option 2 : Supprimer si g√©olocalisation critique\n",
      "    # df_clean = df_clean.dropna(subset=['lat', 'long'])\n",
      "    \n",
      "    # --- COLONNES TEXTUELLES ---\n",
      "    df_clean['v2'].fillna('INCONNU', inplace=True)\n",
      "    df_clean['pr'].fillna('0', inplace=True)\n",
      "    df_clean['libellevoie'].fillna('NON_RENSEIGNE', inplace=True)\n",
      "    \n",
      "    # --- COLONNES NUM√âRIQUES IMPORTANTES ---\n",
      "    # Variables avec peu de manquants (<5%)\n",
      "    colonnes_faible_manquants = ['atm', 'col', 'catr', 'infra', 'circ', 'nbv', \n",
      "                                  'vosp', 'prof', 'plan', 'situ', 'numac']\n",
      "    \n",
      "    for col in colonnes_faible_manquants:\n",
      "        if col in df_clean.columns:\n",
      "            # Remplir avec la m√©diane pour variables num√©riques\n",
      "            if df_clean[col].dtype in ['float64', 'int64']:\n",
      "                df_clean[col].fillna(df_clean[col].median(), inplace=True)\n",
      "    \n",
      "    # --- COLONNES AVEC LOGIQUE M√âTIER ---\n",
      "    # voie : 0 peut signifier \"pas de num√©ro de voie\"\n",
      "    df_clean['voie'].fillna(0, inplace=True)\n",
      "    \n",
      "    # v1 : vitesse - remplir avec m√©diane ou 0 selon contexte\n",
      "    df_clean['v1'].fillna(0, inplace=True)\n",
      "    \n",
      "    # pr1 : point rep√®re - 0 si absent\n",
      "    df_clean['pr1'].fillna(0, inplace=True)\n",
      "    \n",
      "    # typenumero, numero : info adresse\n",
      "    df_clean['typenumero'].fillna(0, inplace=True)\n",
      "    df_clean['numero'].fillna(0, inplace=True)\n",
      "    \n",
      "    # distancemetre : remplir avec 0\n",
      "    df_clean['distancemetre'].fillna(0, inplace=True)\n",
      "    \n",
      "    print(f\"‚úì Nettoyage termin√©\")\n",
      "    return df_clean\n",
      "\n",
      "# Appliquer le nettoyage\n",
      "df_clean = nettoyer_donnees(df)\n",
      "\n",
      "# ========================================\n",
      "# 3. V√âRIFICATION POST-NETTOYAGE\n",
      "# ========================================\n",
      "\n",
      "def verifier_nettoyage(df_clean):\n",
      "    \"\"\"V√©rifier l'√©tat apr√®s nettoyage\"\"\"\n",
      "    print(\"\\n=== √âTAT APR√àS NETTOYAGE ===\")\n",
      "    print(f\"Shape: {df_clean.shape}\")\n",
      "    print(f\"\\nValeurs manquantes restantes:\")\n",
      "    missing_apres = df_clean.isnull().sum()\n",
      "    missing_apres = missing_apres[missing_apres > 0]\n",
      "    \n",
      "    if len(missing_apres) == 0:\n",
      "        print(\"‚úì Aucune valeur manquante !\")\n",
      "    else:\n",
      "        print(missing_apres)\n",
      "    \n",
      "    return df_clean\n",
      "\n",
      "df_clean = verifier_nettoyage(df_clean)\n",
      "\n",
      "# ========================================\n",
      "# 4. ALTERNATIVE : APPROCHE S√âLECTIVE\n",
      "# ========================================\n",
      "\n",
      "def nettoyer_selectif(df, conserver_geo=False):\n",
      "    \"\"\"\n",
      "    Approche alternative selon l'analyse souhait√©e\n",
      "    \n",
      "    conserver_geo: True si analyse g√©ographique n√©cessaire\n",
      "    \"\"\"\n",
      "    df_clean = df.copy()\n",
      "    \n",
      "    if conserver_geo:\n",
      "        # Garder uniquement lignes avec coordonn√©es valides\n",
      "        df_clean = df_clean.dropna(subset=['lat'])\n",
      "        df_clean = df_clean[df_clean['long'].notna()]\n",
      "        print(f\"‚úì Conservation des {len(df_clean)} lignes g√©olocalis√©es\")\n",
      "    \n",
      "    # Supprimer colonnes inutilisables\n",
      "    cols_drop = ['gps', 'coderivoli', 'adr', 'v2']\n",
      "    df_clean = df_clean.drop(columns=[c for c in cols_drop if c in df_clean.columns])\n",
      "    \n",
      "    # Remplir le reste intelligemment\n",
      "    for col in df_clean.columns:\n",
      "        if df_clean[col].isnull().sum() > 0:\n",
      "            if df_clean[col].dtype == 'object':\n",
      "                df_clean[col].fillna('INCONNU', inplace=True)\n",
      "            else:\n",
      "                df_clean[col].fillna(df_clean[col].median(), inplace=True)\n",
      "    \n",
      "    return df_clean\n",
      "\n",
      "# Sauvegarder le r√©sultat\n",
      "# df_clean.to_csv('accidents_nettoyes.csv', index=False)\n",
      "print(\"\\n‚úì Dataset nettoy√© pr√™t pour l'analyse !\")\n",
      "```\n",
      "\n",
      "**Recommandations selon votre usage :**\n",
      "\n",
      "1. **Analyse statistique g√©n√©rale** ‚Üí Utiliser `nettoyer_donnees()`\n",
      "2. **Cartographie** ‚Üí Utiliser `nettoyer_selectif(df, conserver_geo=True)`\n",
      "3. **Machine Learning** ‚Üí Ajouter un encoding des valeurs sentinelles\n",
      "\n",
      "Quelle analyse souhaitez-vous r√©aliser ensuite ?\n",
      "\n",
      "======================================================================\n",
      "QUESTION 5 : Visualisation\n",
      "======================================================================\n",
      "Voici une visualisation pertinente des donn√©es d'accidents de la route :\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import plotly.graph_objects as go\n",
      "import plotly.express as px\n",
      "from plotly.subplots import make_subplots\n",
      "\n",
      "# Supposons que df est votre DataFrame charg√©\n",
      "\n",
      "# ============================================================\n",
      "# 1. Distribution des accidents par gravit√© et luminosit√©\n",
      "# ============================================================\n",
      "\n",
      "# Mapping des codes pour plus de clart√©\n",
      "gravite_map = {0.43: 'Indemne', 0.86: 'Bless√© l√©ger', 10.8: 'Bless√© hospitalis√©', \n",
      "               444188: 'Tu√©'}\n",
      "lum_map = {1: 'Plein jour', 2: 'Cr√©puscule', 3: 'Nuit √©clair√©e', \n",
      "           4: 'Nuit sans √©clairage', 5: '√âclairage non renseign√©'}\n",
      "\n",
      "df_viz = df.copy()\n",
      "df_viz['gravite_label'] = df_viz['grav'].map(gravite_map).fillna('Inconnu')\n",
      "df_viz['lum_label'] = df_viz['lum'].map(lum_map)\n",
      "\n",
      "# Cr√©er une figure avec plusieurs sous-graphiques\n",
      "fig = make_subplots(\n",
      "    rows=2, cols=2,\n",
      "    subplot_titles=('Accidents par gravit√©', \n",
      "                    'Accidents par conditions de luminosit√©',\n",
      "                    'Gravit√© selon la luminosit√©',\n",
      "                    'Top 10 d√©partements les plus touch√©s'),\n",
      "    specs=[[{'type': 'bar'}, {'type': 'bar'}],\n",
      "           [{'type': 'bar'}, {'type': 'bar'}]]\n",
      ")\n",
      "\n",
      "# Graphique 1: Distribution par gravit√©\n",
      "grav_counts = df_viz['gravite_label'].value_counts().sort_index()\n",
      "fig.add_trace(\n",
      "    go.Bar(x=grav_counts.index, y=grav_counts.values, \n",
      "           marker_color='indianred', name='Gravit√©'),\n",
      "    row=1, col=1\n",
      ")\n",
      "\n",
      "# Graphique 2: Distribution par luminosit√©\n",
      "lum_counts = df_viz['lum_label'].value_counts()\n",
      "fig.add_trace(\n",
      "    go.Bar(x=lum_counts.index, y=lum_counts.values,\n",
      "           marker_color='lightblue', name='Luminosit√©'),\n",
      "    row=1, col=2\n",
      ")\n",
      "\n",
      "# Graphique 3: Gravit√© par luminosit√© (heatmap-style)\n",
      "cross_tab = pd.crosstab(df_viz['lum_label'], df_viz['gravite_label'])\n",
      "for col in cross_tab.columns:\n",
      "    fig.add_trace(\n",
      "        go.Bar(x=cross_tab.index, y=cross_tab[col], name=col),\n",
      "        row=2, col=1\n",
      "    )\n",
      "\n",
      "# Graphique 4: Top d√©partements\n",
      "dep_counts = df_viz['dep'].value_counts().head(10)\n",
      "fig.add_trace(\n",
      "    go.Bar(x=dep_counts.index.astype(str), y=dep_counts.values,\n",
      "           marker_color='orange', name='D√©partements'),\n",
      "    row=2, col=2\n",
      ")\n",
      "\n",
      "# Mise en forme\n",
      "fig.update_layout(\n",
      "    height=800,\n",
      "    showlegend=True,\n",
      "    title_text=\"üìä Analyse des accidents de la route - Open Data\",\n",
      "    title_font_size=20\n",
      ")\n",
      "\n",
      "fig.update_xaxes(title_text=\"Gravit√©\", row=1, col=1)\n",
      "fig.update_xaxes(title_text=\"Conditions\", row=1, col=2)\n",
      "fig.update_xaxes(title_text=\"Luminosit√©\", row=2, col=1)\n",
      "fig.update_xaxes(title_text=\"D√©partement\", row=2, col=2)\n",
      "\n",
      "fig.update_yaxes(title_text=\"Nombre d'accidents\", row=1, col=1)\n",
      "fig.update_yaxes(title_text=\"Nombre d'accidents\", row=1, col=2)\n",
      "\n",
      "fig.show()\n",
      "\n",
      "# ============================================================\n",
      "# 2. BONUS - Carte temporelle des accidents (si donn√©es horaires)\n",
      "# ============================================================\n",
      "\n",
      "# Distribution par type d'usager (si interpr√©table)\n",
      "fig2 = px.histogram(df_viz, x='lum_label', color='gravite_label',\n",
      "                    title='Distribution d√©taill√©e: Gravit√© vs Luminosit√©',\n",
      "                    labels={'lum_label': 'Conditions de luminosit√©',\n",
      "                            'gravite_label': 'Gravit√©'},\n",
      "                    barmode='group',\n",
      "                    height=500)\n",
      "\n",
      "fig2.update_layout(xaxis_tickangle=-45)\n",
      "fig2.show()\n",
      "\n",
      "# ============================================================\n",
      "# 3. Analyse des victimes\n",
      "# ============================================================\n",
      "\n",
      "victimes_df = pd.DataFrame({\n",
      "    'Type': ['Tu√©s', 'Bless√©s graves', 'Bless√©s l√©gers', 'Indemnes'],\n",
      "    'Nombre': [df['ttue'].sum(), df['tbg'].sum(), df['tbl'].sum(), df['tindm'].sum()]\n",
      "})\n",
      "\n",
      "fig3 = px.pie(victimes_df, values='Nombre', names='Type',\n",
      "              title='R√©partition des victimes',\n",
      "              color_discrete_sequence=px.colors.sequential.RdBu)\n",
      "fig3.show()\n",
      "```\n",
      "\n",
      "**Points cl√©s de cette visualisation :**\n",
      "\n",
      "1. **Dashboard multi-graphiques** : Vue d'ensemble avec 4 perspectives\n",
      "2. **Analyse gravit√©** : Distribution des niveaux de blessures\n",
      "3. **Conditions de luminosit√©** : Impact sur les accidents\n",
      "4. **Analyse crois√©e** : Gravit√© selon luminosit√© (barres empil√©es)\n",
      "5. **G√©ographie** : Top d√©partements touch√©s\n",
      "6. **Victimes** : R√©partition en camembert\n",
      "\n",
      "**Pour aller plus loin :**\n",
      "```python\n",
      "# Analyse temporelle si vous avez des donn√©es de date\n",
      "# Carte g√©ographique si lat/long exploitables\n",
      "# Analyse des types de routes (catr) vs gravit√©\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Partie 2.3 : Exploration assist√©e par IA\n",
    "# Posage des 5 questions principales sur le dataset\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"QUESTION 1 : Description du dataset\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "q1 = ask_data_assistant(\n",
    "    \"D√©cris ce dataset en 3 phrases. Quel est son sujet principal ?\",\n",
    "    dataset_context\n",
    ")\n",
    "print(q1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"QUESTION 2 : Probl√®mes de qualit√© de donn√©es\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "q2 = ask_data_assistant(\n",
    "    \"Quels sont les 5 principaux probl√®mes de qualit√© de donn√©es que tu vois ?\",\n",
    "    dataset_context\n",
    ")\n",
    "print(q2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"QUESTION 3 : Proposer des analyses\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "q3 = ask_data_assistant(\n",
    "    \"Propose 3 analyses int√©ressantes qu'on pourrait faire avec ces donn√©es sur les accidents.\",\n",
    "    dataset_context\n",
    ")\n",
    "print(q3)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"QUESTION 4 : Code de nettoyage\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "q4 = ask_data_assistant(\n",
    "    \"G√©n√®re le code Python pour nettoyer les valeurs manquantes de mani√®re appropri√©e.\",\n",
    "    dataset_context\n",
    ")\n",
    "print(q4)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"QUESTION 5 : Visualisation\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "q5 = ask_data_assistant(\n",
    "    \"G√©n√®re le code Plotly pour cr√©er une visualisation pertinente des donn√©es.\",\n",
    "    dataset_context\n",
    ")\n",
    "print(q5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f503f6da-074e-4725-9db5-7415c36e2fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "QUESTION 6 : Question libre 1\n",
      "======================================================================\n",
      "# Analyse des d√©partements avec le plus d'accidents\n",
      "\n",
      "Je vais t'aider √† identifier les d√©partements les plus accidentog√®nes. Voici comment proc√©der :\n",
      "\n",
      "## 1. Code pour identifier les top d√©partements\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import plotly.express as px\n",
      "\n",
      "# Compter les accidents par d√©partement\n",
      "accidents_par_dep = df['dep'].value_counts().reset_index()\n",
      "accidents_par_dep.columns = ['departement', 'nb_accidents']\n",
      "\n",
      "# Top 10 d√©partements\n",
      "top10 = accidents_par_dep.head(10)\n",
      "print(top10)\n",
      "\n",
      "# Visualisation\n",
      "fig = px.bar(top10, \n",
      "             x='departement', \n",
      "             y='nb_accidents',\n",
      "             title='Top 10 des d√©partements avec le plus d\\'accidents',\n",
      "             labels={'departement': 'D√©partement', 'nb_accidents': 'Nombre d\\'accidents'})\n",
      "fig.show()\n",
      "```\n",
      "\n",
      "## 2. Analyse enrichie avec taux d'accidents\n",
      "\n",
      "```python\n",
      "# Si tu as les populations d√©partementales\n",
      "populations = {\n",
      "    # Exemple: '75': 2_165_000, '13': 2_044_000, etc.\n",
      "}\n",
      "\n",
      "# Calcul du taux pour 100k habitants\n",
      "accidents_par_dep['taux_100k'] = (\n",
      "    accidents_par_dep['departement']\n",
      "    .map(populations)\n",
      "    .apply(lambda pop: (accidents_par_dep['nb_accidents'] / pop * 100000) if pd.notna(pop) else None)\n",
      ")\n",
      "```\n",
      "\n",
      "## 3. Analyse par gravit√©\n",
      "\n",
      "```python\n",
      "# Croiser d√©partement et gravit√©\n",
      "dep_gravite = df.groupby(['dep', 'grav']).size().reset_index(name='count')\n",
      "\n",
      "# Accidents graves (grav > 2) par d√©partement\n",
      "accidents_graves = df[df['grav'] > 2].groupby('dep').size().sort_values(ascending=False).head(10)\n",
      "print(\"D√©partements avec le plus d'accidents graves:\")\n",
      "print(accidents_graves)\n",
      "```\n",
      "\n",
      "## 4. Caract√©ristiques des d√©partements √† risque\n",
      "\n",
      "```python\n",
      "# Profil des accidents par d√©partement\n",
      "profil_dep = df.groupby('dep').agg({\n",
      "    'grav': 'mean',  # Gravit√© moyenne\n",
      "    'lum': lambda x: (x == 5).sum() / len(x),  # % accidents de nuit\n",
      "    'atm': lambda x: (x > 1).sum() / len(x),  # % mauvaises conditions\n",
      "    'agg': lambda x: (x <= 2).sum() / len(x)  # % en agglom√©ration\n",
      "}).round(3)\n",
      "\n",
      "# Top 5 d√©partements\n",
      "top5_deps = accidents_par_dep.head(5)['departement'].tolist()\n",
      "print(profil_dep.loc[top5_deps])\n",
      "```\n",
      "\n",
      "## 5. Cartographie (si coordonn√©es disponibles)\n",
      "\n",
      "```python\n",
      "# Carte des accidents par d√©partement\n",
      "dep_coords = df.groupby('dep').agg({\n",
      "    'lat': 'mean',\n",
      "    'long': lambda x: pd.to_numeric(x, errors='coerce').mean(),\n",
      "    'grav': 'count'\n",
      "}).dropna()\n",
      "\n",
      "fig = px.scatter_geo(dep_coords.reset_index(),\n",
      "                     lat='lat',\n",
      "                     lon='long',\n",
      "                     size='grav',\n",
      "                     hover_name='dep',\n",
      "                     title='R√©partition g√©ographique des accidents')\n",
      "fig.show()\n",
      "```\n",
      "\n",
      "## Points d'attention\n",
      "\n",
      "**Avec tes donn√©es :**\n",
      "- **439 332 accidents** au total\n",
      "- D√©partements de **10 √† 974** (incluant DOM-TOM)\n",
      "- ~78% ont des coordonn√©es GPS disponibles\n",
      "\n",
      "**Facteurs √† consid√©rer :**\n",
      "- **Population** : d√©partements urbains vs ruraux\n",
      "- **R√©seau routier** : longueur, type d'infrastructures\n",
      "- **Conditions m√©t√©o** : variable `atm`\n",
      "- **Type de route** : variable `catr` (autoroute, nationale, etc.)\n",
      "\n",
      "Veux-tu que je d√©taille l'analyse sur un aspect sp√©cifique ?\n",
      "\n",
      "======================================================================\n",
      "QUESTION 7 : Question libre 2\n",
      "======================================================================\n",
      "# Analyse de la relation Gravit√© vs Conditions m√©t√©orologiques\n",
      "\n",
      "Voici une approche compl√®te pour analyser cette relation :\n",
      "\n",
      "## 1. üìä Analyse descriptive initiale\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import plotly.express as px\n",
      "import plotly.graph_objects as go\n",
      "\n",
      "# Exploration des valeurs\n",
      "print(\"Distribution de la gravit√©:\")\n",
      "print(df['grav'].value_counts().sort_index())\n",
      "print(\"\\nDistribution des conditions atmosph√©riques:\")\n",
      "print(df['atm'].value_counts().sort_index())\n",
      "\n",
      "# Tableau crois√©\n",
      "tableau_croise = pd.crosstab(df['atm'], df['grav'], \n",
      "                             normalize='index', margins=True) * 100\n",
      "print(\"\\nPourcentage de gravit√© par condition m√©t√©o:\")\n",
      "print(tableau_croise.round(2))\n",
      "```\n",
      "\n",
      "## 2. üìà Visualisations recommand√©es\n",
      "\n",
      "### A) Graphique en barres group√©es\n",
      "```python\n",
      "# Cr√©er un r√©sum√©\n",
      "resume = df.groupby(['atm', 'grav']).size().reset_index(name='count')\n",
      "\n",
      "fig = px.bar(resume, x='atm', y='count', color='grav',\n",
      "             title='R√©partition de la gravit√© des accidents par conditions m√©t√©o',\n",
      "             labels={'atm': 'Conditions atmosph√©riques', \n",
      "                     'count': 'Nombre d\\'accidents',\n",
      "                     'grav': 'Gravit√©'},\n",
      "             barmode='group')\n",
      "fig.show()\n",
      "```\n",
      "\n",
      "### B) Heatmap pour mieux voir les patterns\n",
      "```python\n",
      "# Cr√©er une matrice de contingence\n",
      "matrice = pd.crosstab(df['atm'], df['grav'])\n",
      "\n",
      "fig = px.imshow(matrice, \n",
      "                labels=dict(x=\"Gravit√©\", y=\"Conditions atmosph√©riques\", \n",
      "                           color=\"Nombre d'accidents\"),\n",
      "                title=\"Heatmap: Gravit√© vs Conditions m√©t√©o\",\n",
      "                text_auto=True)\n",
      "fig.show()\n",
      "```\n",
      "\n",
      "## 3. üîç Tests statistiques\n",
      "\n",
      "```python\n",
      "from scipy.stats import chi2_contingency\n",
      "\n",
      "# Test du Chi-2 pour v√©rifier l'ind√©pendance\n",
      "contingency_table = pd.crosstab(df['atm'].dropna(), df['grav'])\n",
      "chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
      "\n",
      "print(f\"Test du Chi-2:\")\n",
      "print(f\"Statistique: {chi2:.2f}\")\n",
      "print(f\"P-value: {p_value:.4e}\")\n",
      "print(f\"Degr√©s de libert√©: {dof}\")\n",
      "\n",
      "if p_value < 0.05:\n",
      "    print(\"‚úì Il existe une relation significative entre m√©t√©o et gravit√©\")\n",
      "else:\n",
      "    print(\"‚úó Pas de relation significative d√©tect√©e\")\n",
      "```\n",
      "\n",
      "## 4. üìä Analyse des proportions\n",
      "\n",
      "```python\n",
      "# Calculer les taux de gravit√© √©lev√©e par condition\n",
      "df['grave'] = df['grav'] >= 3  # D√©finir un seuil de gravit√©\n",
      "\n",
      "taux_gravite = df.groupby('atm')['grave'].agg(['mean', 'count'])\n",
      "taux_gravite['mean'] = taux_gravite['mean'] * 100\n",
      "taux_gravite.columns = ['Taux_gravit√©_√©lev√©e_%', 'Nombre_accidents']\n",
      "\n",
      "print(\"\\nTaux d'accidents graves par condition m√©t√©o:\")\n",
      "print(taux_gravite.sort_values('Taux_gravit√©_√©lev√©e_%', ascending=False))\n",
      "\n",
      "# Visualisation\n",
      "fig = px.bar(taux_gravite.reset_index(), \n",
      "             x='atm', y='Taux_gravit√©_√©lev√©e_%',\n",
      "             title='Taux d\\'accidents graves par condition m√©t√©o',\n",
      "             labels={'atm': 'Conditions atmosph√©riques'})\n",
      "fig.show()\n",
      "```\n",
      "\n",
      "## 5. üéØ Analyse multivari√©e (avec autres facteurs)\n",
      "\n",
      "```python\n",
      "# Ajouter d'autres variables contextuelles\n",
      "analyse_complete = df.groupby(['atm', 'lum']).agg({\n",
      "    'grav': ['mean', 'count']\n",
      "}).reset_index()\n",
      "\n",
      "analyse_complete.columns = ['atm', 'lum', 'gravite_moyenne', 'nb_accidents']\n",
      "\n",
      "# Graphique √† bulles\n",
      "fig = px.scatter(analyse_complete, \n",
      "                 x='atm', y='gravite_moyenne',\n",
      "                 size='nb_accidents', color='lum',\n",
      "                 title='Gravit√© selon m√©t√©o et luminosit√©',\n",
      "                 labels={'atm': 'Conditions m√©t√©o', \n",
      "                        'gravite_moyenne': 'Gravit√© moyenne'})\n",
      "fig.show()\n",
      "```\n",
      "\n",
      "## üí° Points cl√©s √† interpr√©ter\n",
      "\n",
      "1. **Significativit√© statistique** : Le test du Chi-2 indique si la relation est r√©elle\n",
      "2. **Taille de l'effet** : Comparer les % d'accidents graves entre conditions\n",
      "3. **Volume** : Attention aux conditions rares (faible effectif)\n",
      "4. **Facteurs confondants** : Croiser avec luminosit√©, type de route, etc.\n",
      "\n",
      "Quelle partie veux-tu approfondir ?\n",
      "\n",
      "======================================================================\n",
      "QUESTION 8 : Question libre 3\n",
      "======================================================================\n",
      "# Analyse pour la Pr√©vention des Accidents Graves\n",
      "\n",
      "Voici les insights cl√©s bas√©s sur les donn√©es d'accidents :\n",
      "\n",
      "## üîç Observations Principales\n",
      "\n",
      "### 1. **Gravit√© des Accidents (colonne `grav`)**\n",
      "- Moyenne de 12.46 ‚Üí forte concentration sur certains niveaux de gravit√©\n",
      "- 439,332 accidents avec des variations importantes\n",
      "\n",
      "### 2. **Facteurs Temporels Critiques**\n",
      "\n",
      "```python\n",
      "# Analyse de la luminosit√© (lum)\n",
      "import pandas as pd\n",
      "import plotly.express as px\n",
      "\n",
      "# Distribution luminosit√© vs gravit√©\n",
      "df_grouped = df.groupby('lum').agg({\n",
      "    'grav': 'mean',\n",
      "    'numac': 'count'\n",
      "}).reset_index()\n",
      "\n",
      "# 1=Jour, 2=Cr√©puscule, 3=Nuit sans √©clairage, 4=Nuit avec √©clairage, 5=Non d√©fini\n",
      "```\n",
      "\n",
      "**Insight** : `lum` moyenne = 1.92 ‚Üí beaucoup d'accidents de jour, mais analyser la gravit√© selon conditions lumineuses\n",
      "\n",
      "### 3. **Profil D√©mographique des Victimes**\n",
      "\n",
      "```python\n",
      "# Analyse des types de victimes\n",
      "print(f\"Tu√©s: {df['ttue'].sum()}\")\n",
      "print(f\"Bless√©s graves: {df['tbg'].sum()}\")  \n",
      "print(f\"Bless√©s l√©gers: {df['tbl'].sum()}\")\n",
      "\n",
      "# Ratio gravit√©\n",
      "df['ratio_graves'] = (df['ttue'] + df['tbg']) / (df['ttue'] + df['tbg'] + df['tbl'])\n",
      "```\n",
      "\n",
      "**Moyennes** : \n",
      "- Tu√©s: 0.06/accident\n",
      "- Bless√©s graves: 0.47/accident  \n",
      "- Bless√©s l√©gers: 0.78/accident\n",
      "\n",
      "### 4. **Infrastructure Dangereuse**\n",
      "\n",
      "```python\n",
      "# Top d√©partements √† risque\n",
      "top_dep = df.groupby('dep').agg({\n",
      "    'ttue': 'sum',\n",
      "    'tbg': 'sum',\n",
      "    'numac': 'count'\n",
      "}).sort_values('ttue', ascending=False).head(10)\n",
      "\n",
      "# Types de routes (catr)\n",
      "route_risk = df.groupby('catr')['grav'].mean().sort_values(ascending=False)\n",
      "```\n",
      "\n",
      "### 5. **Conditions Atmosph√©riques**\n",
      "\n",
      "```python\n",
      "# Impact m√©t√©o (atm: 1=Normal, 2-9=Conditions d√©grad√©es)\n",
      "meteo_analysis = df.groupby('atm').agg({\n",
      "    'grav': 'mean',\n",
      "    'ttue': 'mean',\n",
      "    'numac': 'count'\n",
      "})\n",
      "```\n",
      "\n",
      "**Attention** : `atm` moyenne = 1.54 ‚Üí majorit√© en conditions normales, mais analyser les pics en conditions d√©grad√©es\n",
      "\n",
      "## üéØ Recommandations Prioritaires\n",
      "\n",
      "### Code d'Analyse Approfondie\n",
      "\n",
      "```python\n",
      "# 1. Identifier les zones noires\n",
      "zones_critiques = df.groupby(['dep', 'com']).agg({\n",
      "    'ttue': 'sum',\n",
      "    'tbg': 'sum',\n",
      "    'numac': 'count',\n",
      "    'grav': 'mean'\n",
      "}).query('ttue > 5 or tbg > 20').sort_values('ttue', ascending=False)\n",
      "\n",
      "# 2. Croisement facteurs de risque\n",
      "import seaborn as sns\n",
      "risk_matrix = pd.crosstab(\n",
      "    index=df['lum'], \n",
      "    columns=df['atm'], \n",
      "    values=df['grav'], \n",
      "    aggfunc='mean'\n",
      ")\n",
      "\n",
      "# 3. Analyse temporelle (si donn√©es disponibles)\n",
      "# V√©rifier colonnes temporelles pour patterns horaires/saisonniers\n",
      "\n",
      "# 4. Types de collision (col)\n",
      "collision_severity = df.groupby('col').agg({\n",
      "    'grav': 'mean',\n",
      "    'ttue': 'sum',\n",
      "    'numac': 'count'\n",
      "}).sort_values('grav', ascending=False)\n",
      "\n",
      "# 5. Intersections vs Routes (int)\n",
      "intersection_risk = df.groupby('int')['grav'].agg(['mean', 'count'])\n",
      "```\n",
      "\n",
      "## üìä Insights Actionnables\n",
      "\n",
      "1. **Zones √† Surveiller** : D√©partements 630, 780, 320 (selon distribution)\n",
      "2. **Conditions Critiques** : Croiser `lum` √ó `atm` √ó `catr` pour identifier les combinaisons mortelles\n",
      "3. **Usagers Vuln√©rables** : Ratio √©lev√© tbl/tbg ‚Üí cibler pr√©vention bless√©s l√©gers\n",
      "4. **Infrastructure** : 32% de donn√©es manquantes sur `voie` ‚Üí compl√©ter pour analyse spatiale\n",
      "\n",
      "## üö® Alertes Importantes\n",
      "\n",
      "- **289 d√©partements** diff√©rents ‚Üí analyse g√©ographique essentielle\n",
      "- **‚âà50% de donn√©es GPS manquantes** ‚Üí limiter cartographie pr√©cise\n",
      "- **Valeurs extr√™mes** dans `numero` et `voie` ‚Üí nettoyer avant analyse\n",
      "\n",
      "**Prochaine √©tape recommand√©e** : Quelle dimension veux-tu explorer en priorit√© ?\n"
     ]
    }
   ],
   "source": [
    "# Partie 2.4 : Exercice libre - Questions suppl√©mentaires\n",
    "# Exploration cr√©ative du dataset avec 3 questions personnalis√©es\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"QUESTION 6 : Question libre 1\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "q6 = ask_data_assistant(\n",
    "    \"Quels sont les d√©partements (dep) avec le plus d'accidents et comment pourrait-on les identifier ?\",\n",
    "    dataset_context\n",
    ")\n",
    "print(q6)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"QUESTION 7 : Question libre 2\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "q7 = ask_data_assistant(\n",
    "    \"Comment pourrait-on analyser la relation entre la gravit√© des accidents et les conditions m√©t√©orologiques (colonne atm) ?\",\n",
    "    dataset_context\n",
    ")\n",
    "print(q7)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"QUESTION 8 : Question libre 3\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "q8 = ask_data_assistant(\n",
    "    \"Quels insights int√©ressants pourraient aider √† pr√©venir les accidents graves dans ce dataset ?\",\n",
    "    dataset_context\n",
    ")\n",
    "print(q8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9cb6bba-d572-4c6f-8a0a-53c43d8b0d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G√©n√©ration de la fiche technique...\n",
      "\n",
      "# Accidents corporels de la circulation routi√®re en France\n",
      "\n",
      "## Informations g√©n√©rales\n",
      "- **Source** : data.gouv.fr\n",
      "- **Licence** : Licence Ouverte (√† v√©rifier sur le portail)\n",
      "- **Nombre de lignes** : 439 332\n",
      "- **Nombre de colonnes** : 36\n",
      "- **P√©riode couverte** : Plusieurs ann√©es (√† d√©terminer selon les donn√©es `org`)\n",
      "\n",
      "## Description\n",
      "\n",
      "Ce dataset recense les **accidents corporels de la circulation routi√®re** survenus en France m√©tropolitaine et dans les d√©partements d'outre-mer. Chaque ligne correspond √† un accident ayant fait au moins une victime (d√©c√©d√©, bless√© hospitalis√© ou bless√© l√©ger). Les donn√©es sont collect√©es par les forces de l'ordre √† partir des proc√®s-verbaux d'accidents.\n",
      "\n",
      "Le fichier contient des informations d√©taill√©es sur les **circonstances des accidents** (conditions m√©t√©orologiques, luminosit√©, type de collision), la **localisation g√©ographique** (d√©partement, commune, adresse, coordonn√©es GPS) et les **cons√©quences humaines** (nombre de tu√©s, bless√©s graves, bless√©s l√©gers). Ces donn√©es sont issues du fichier caract√©ristiques des accidents du Fichier National des Accidents Corporels (BAAC).\n",
      "\n",
      "Les informations permettent d'analyser les facteurs de risque routier, d'identifier les zones accidentog√®nes et d'√©valuer l'impact des politiques de s√©curit√© routi√®re. La richesse des variables offre des possibilit√©s d'analyse crois√©e entre conditions m√©t√©orologiques, infrastructure routi√®re et gravit√© des accidents.\n",
      "\n",
      "## Structure des donn√©es\n",
      "\n",
      "| Colonne | Type | Description | Valeurs manquantes |\n",
      "|---------|------|-------------|-------------------|\n",
      "| **org** | int64 | Organisme ayant saisi l'accident (1-5) | 0 |\n",
      "| **lum** | int64 | Conditions d'√©clairage (1:jour, 2:cr√©puscule, 3:nuit √©clair√©e, 4:nuit non √©clair√©e, 5:inconnu) | 0 |\n",
      "| **agg** | int64 | Localisation (1:hors agglo, 2:en agglo) | 0 |\n",
      "| **int** | int64 | Type d'intersection (0:hors intersection, 1-9:types vari√©s) | 0 |\n",
      "| **atm** | float64 | Conditions atmosph√©riques (1:normale, 2-9:pluie, neige, brouillard...) | 23 |\n",
      "| **col** | float64 | Type de collision (1-7:frontale, arri√®re, lat√©rale...) | 9 |\n",
      "| **com** | int64 | Code commune INSEE | 0 |\n",
      "| **dep** | int64 | Code d√©partement | 0 |\n",
      "| **catr** | float64 | Cat√©gorie de route (1:autoroute, 2:nationale, 3:d√©partementale...) | 13 673 |\n",
      "| **infra** | float64 | Infrastructure sp√©ciale (0:normale, 1-7:pont, tunnel...) | 14 014 |\n",
      "| **voie** | float64 | Num√©ro de route | 32 254 |\n",
      "| **v1** | float64 | Indice num√©rique de route | 211 832 |\n",
      "| **v2** | object | Lettre indice de route | 418 106 |\n",
      "| **circ** | float64 | R√©gime de circulation (1:sens unique, 2:bidirectionnel...) | 13 885 |\n",
      "| **nbv** | float64 | Nombre de voies | 14 540 |\n",
      "| **pr** | object | Point rep√®re (PR) | 219 717 |\n",
      "| **pr1** | float64 | Distance au PR | 220 106 |\n",
      "| **vosp** | float64 | Voie r√©serv√©e (0:non, 1-3:piste cyclable, couloir bus...) | 14 205 |\n",
      "| **prof** | float64 | Profil en long (1:plat, 2:pente, 3:sommet...) | 13 910 |\n",
      "| **plan** | float64 | Trac√© en plan (1:rectiligne, 2:courbe...) | 13 909 |\n",
      "| **situ** | float64 | Situation de l'accident (1:chauss√©e, 2:accotement...) | 13 857 |\n",
      "| **ttue** | int64 | **Nombre de tu√©s** | 0 |\n",
      "| **tbg** | int64 | **Nombre de bless√©s graves** | 0 |\n",
      "| **tbl** | int64 | **Nombre de bless√©s l√©gers** | 0 |\n",
      "| **tindm** | int64 | **Nombre d'indemnes** | 0 |\n",
      "| **typenumero** | float64 | Type de num√©ro de voie | 124 888 |\n",
      "| **numero** | float64 | Num√©ro dans la voie | 239 579 |\n",
      "| **distancemetre** | float64 | Distance en m√®tres | 127 124 |\n",
      "| **libellevoie** | object | Nom de la voie | 124 590 |\n",
      "| **coderivoli** | object | Code RIVOLI de la voie | 439 190 |\n",
      "| **grav** | float64 | Gravit√© maximale de l'accident (0.43-444188) | 0 |\n",
      "| **gps** | object | Coordonn√©es GPS combin√©es | 434 814 |\n",
      "| **lat** | float64 | Latitude | 279 783 |\n",
      "| **long** | object | Longitude | 279 856 |\n",
      "| **adr** | object | Adresse compl√®te | 428 287 |\n",
      "| **numac** | float64 | Num√©ro d'accident unique | 1 |\n",
      "\n",
      "## Qualit√© des donn√©es\n",
      "\n",
      "### Points forts\n",
      "- **Exhaustivit√© des accidents corporels** : base nationale compl√®te avec 439 332 enregistrements\n",
      "- **Variables de gravit√© compl√®tes** : ttue, tbg, tbl sans valeurs manquantes\n",
      "- **Identification unique** : colonne `numac` quasi-compl√®te (1 seule valeur manquante)\n",
      "- **Couverture g√©ographique** : codes d√©partement et commune complets\n",
      "- **Variables temporelles et contextuelles** : luminosit√©, m√©t√©o bien renseign√©es\n",
      "\n",
      "### Points d'attention\n",
      "- **G√©olocalisation incompl√®te** : 63,7% de valeurs manquantes pour lat/long, 99% pour gps\n",
      "- **Adresses partielles** : 97,5% de valeurs manquantes pour `adr`\n",
      "- **Code RIVOLI quasi-absent** : 99,96% de valeurs manquantes\n",
      "- **Variables de route incompl√®tes** : v1 (48%), v2 (95%), pr (50%)\n",
      "- **Valeurs aberrantes** : \n",
      "  - `numero` avec valeurs extr√™mes (8e+280)\n",
      "  - `voie` jusqu'√† 90 000\n",
      "  - `grav` avec des valeurs incoh√©rentes (jusqu'√† 444 188)\n",
      "- **Types incoh√©rents** : `long` en object au lieu de float64\n",
      "\n",
      "## Cas d'usage potentiels\n",
      "\n",
      "1. **Analyse de la s√©curit√© routi√®re**\n",
      "   - Identification des zones √† risque (black-spots)\n",
      "   - Corr√©lation m√©t√©o/luminosit√© et gravit√© des accidents\n",
      "   - √âvolution temporelle de l'accidentalit√©\n",
      "\n",
      "2. **Cartographie des accidents**\n",
      "   - Heatmaps des accidents par d√©partement/commune\n",
      "   - Visualisation des axes routiers dangereux\n",
      "   - Zonage pour priorisation des am√©nagements\n",
      "\n",
      "3. **√âtudes pr√©dictives**\n",
      "   - Mod√©lisation de la gravit√© selon les conditions\n",
      "   - Facteurs de risque (intersection, type de route)\n",
      "   - Pr√©vision saisonni√®re des accidents\n",
      "\n",
      "4. **Politique publique**\n",
      "   - √âvaluation de l'impact des radars automatiques\n",
      "   - Ciblage des campagnes de pr√©vention\n",
      "   - Allocation des ressources de secours\n",
      "\n",
      "5. **Recherche acad√©mique**\n",
      "   - Impact des infrastructures sur la s√©curit√©\n",
      "   - Comportements √† risque selon contexte\n",
      "   - Analyses socio-d√©mographiques (√† croiser avec d'autres datasets)\n",
      "\n",
      "## Analyse recommand√©e\n",
      "\n",
      "### Nettoyage prioritaire\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# Charger les donn√©es\n",
      "df = pd.read_csv('accidents.csv')\n",
      "\n",
      "# Corriger le type de 'long'\n",
      "df['long'] = pd.to_numeric(df['long'], errors='coerce')\n",
      "\n",
      "# Filtrer valeurs aberrantes de 'grav'\n",
      "df = df[df['grav'] < 100]  # Valeur √† ajuster selon documentation\n",
      "\n",
      "# Cr√©er variable gravit√© simplifi√©e\n",
      "df['avec_tue'] = (df['ttue'] > 0).astype(int)\n",
      "df['total_victimes'] = df['ttue'] + df['tbg'] + df['tbl']\n",
      "```\n",
      "\n",
      "### Analyses essentielles\n",
      "\n",
      "**1. Vue d'ensemble de l'accidentalit√©**\n",
      "```python\n",
      "# Distribution de la gravit√©\n",
      "print(f\"Accidents avec tu√©(s) : {df['ttue'].gt(0).sum()}\")\n",
      "print(f\"Total de tu√©s : {df['ttue'].sum()}\")\n",
      "print(f\"Total de bless√©s : {(df['tbg'] + df['tbl']).sum()}\")\n",
      "\n",
      "# Top d√©partements\n",
      "top_dept = df.groupby('dep')['numac'].count().sort_values(ascending=False).head(10)\n",
      "```\n",
      "\n",
      "**2. Facteurs de risque**\n",
      "```python\n",
      "# Luminosit√© et gravit√©\n",
      "df.groupby('lum')['ttue'].agg(['sum', 'mean'])\n",
      "\n",
      "# M√©t√©o et accidents mortels\n",
      "df[df['ttue'] > 0].groupby('atm').size()\n",
      "```\n",
      "\n",
      "**3. Cartographie (pour donn√©es g√©olocalis√©es)**\n",
      "```python\n",
      "import plotly.express as px\n",
      "\n",
      "# Filtrer donn√©es avec coordonn√©es\n",
      "df_geo = df.dropna(subset=['lat', 'long'])\n",
      "df_geo = df_geo[(df_geo['lat'] > 41) & (df_geo['lat'] < 52)]\n",
      "\n",
      "# Carte des accidents mortels\n",
      "fig = px.scatter_mapbox(\n",
      "    df_geo[df_geo['ttue'] > 0],\n",
      "    lat='lat', lon='long',\n",
      "    hover_name='dep',\n",
      "    zoom=5\n",
      ")\n",
      "```\n",
      "\n",
      "### Recommandations m√©thodologiques\n",
      "- **Privil√©gier les analyses agr√©g√©es** (par d√©partement/commune) plut√¥t que ponctuelles vu les donn√©es GPS partielles\n",
      "- **V√©rifier la documentation officielle** pour d√©coder les variables cat√©gorielles (lum, atm, col, etc.)\n",
      "- **Croiser avec d'autres datasets** : donn√©es de trafic, m√©t√©o d√©taill√©e, caract√©ristiques d√©mographiques\n",
      "- **Attention √† la temporalit√©** : identifier les ann√©es couvertes via des m√©tadonn√©es ou patterns dans les donn√©es\n"
     ]
    }
   ],
   "source": [
    "# Partie 3.1 : G√©n√©ration de la fiche technique\n",
    "# Demande √† Claude de g√©n√©rer une documentation compl√®te du dataset\n",
    "\n",
    "print(\"G√©n√©ration de la fiche technique...\")\n",
    "print()\n",
    "\n",
    "# Prompt pour g√©n√©rer la fiche technique au format Markdown\n",
    "fiche_prompt = f\"\"\"\n",
    "G√©n√®re une fiche technique compl√®te pour ce dataset au format Markdown.\n",
    "\n",
    "Structure attendue :\n",
    "# Nom du dataset\n",
    "\n",
    "## Informations g√©n√©rales\n",
    "- Source : data.gouv.fr\n",
    "- Licence : \n",
    "- Nombre de lignes : {len(df)}\n",
    "- Nombre de colonnes : {len(df.columns)}\n",
    "\n",
    "## Description\n",
    "[Description en 2-3 paragraphes du contenu]\n",
    "\n",
    "## Structure des donn√©es\n",
    "[Tableau avec colonnes, types et descriptions]\n",
    "\n",
    "## Qualit√© des donn√©es\n",
    "### Points forts\n",
    "- ...\n",
    "\n",
    "### Points d'attention\n",
    "- ...\n",
    "\n",
    "## Cas d'usage potentiels\n",
    "1. ...\n",
    "2. ...\n",
    "3. ...\n",
    "\n",
    "## Analyse recommand√©e\n",
    "[Recommandations pour exploiter le dataset]\n",
    "\n",
    "Voici le contexte du dataset :\n",
    "{dataset_context}\n",
    "\"\"\"\n",
    "\n",
    "# Appel √† Claude pour g√©n√©rer la fiche\n",
    "fiche_technique = ask_data_assistant(fiche_prompt)\n",
    "\n",
    "print(fiche_technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37d03d4a-7a56-4017-a0a0-1f9df27451fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fiche technique sauvegard√©e dans FICHE_TECHNIQUE.md !\n",
      "\n",
      "Fichier cr√©√© avec succ√®s dans le dossier du projet.\n"
     ]
    }
   ],
   "source": [
    "# Partie 3.2 : Sauvegarde de la fiche technique\n",
    "# Enregistrement de la documentation g√©n√©r√©e en fichier Markdown\n",
    "\n",
    "# Sauvegarde de la fiche technique en fichier .md\n",
    "with open('FICHE_TECHNIQUE.md', 'w', encoding='utf-8') as fichier:\n",
    "    fichier.write(fiche_technique)\n",
    "\n",
    "print(\"Fiche technique sauvegard√©e dans FICHE_TECHNIQUE.md !\")\n",
    "print()\n",
    "print(\"Fichier cr√©√© avec succ√®s dans le dossier du projet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23b1df0b-01c4-42dc-8249-143898f7acc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md cr√©√© avec succ√®s !\n"
     ]
    }
   ],
   "source": [
    "# Partie 3.3 : Cr√©ation du README.md\n",
    "# Documentation pour expliquer comment reproduire le TP\n",
    "\n",
    "readme_content = \"\"\"# TP1 - Exploration augment√©e d'un dataset Open Data\n",
    "\n",
    "## Dataset choisi\n",
    "Accidents corporels de la circulation routi√®re (data.gouv.fr)\n",
    "- Source : https://www.data.gouv.fr/fr/datasets/bases-de-donnees-annuelles-des-accidents-corporels-de-la-circulation-routiere-annees-de-2005-a-2024/\n",
    "- Nombre de lignes : 439 332\n",
    "- Nombre de colonnes : 36\n",
    "\n",
    "## Description\n",
    "Ce TP explore un dataset Open Data contenant les informations sur les accidents corporels survenus sur les routes fran√ßaises. L'exploration est assist√©e par Claude Sonnet 4.5 pour identifier les caract√©ristiques, probl√®mes de qualit√© et analyses potentielles.\n",
    "\n",
    "## Installation\n",
    "```bash\n",
    "cd tp1-exploration\n",
    "uv sync\n",
    "```\n",
    "\n",
    "## Configuration\n",
    "1. Cr√©er un fichier `.env` √† la racine du projet\n",
    "2. Ajouter la cl√© API Anthropic :\n",
    "```env\n",
    "ANTHROPIC_API_KEY=cl√©_ici\n",
    "```\n",
    "\n",
    "## Utilisation\n",
    "```bash\n",
    "uv run jupyter notebook\n",
    "```\n",
    "Ouvrez le fichier `exploration.ipynb`\n",
    "\n",
    "## Fichiers du projet\n",
    "- `exploration.ipynb` : Notebook principal avec l'exploration du dataset\n",
    "- `FICHE_TECHNIQUE.md` : Documentation d√©taill√©e du dataset\n",
    "- `accidents.csv` : Fichier de donn√©es (√† t√©l√©charger depuis data.gouv.fr)\n",
    "\n",
    "## Contenu du TP\n",
    "- Partie 0 : Setup de l'environnement\n",
    "- Partie 1 : Chargement du dataset\n",
    "- Partie 2 : Exploration assist√©e par IA (8 questions)\n",
    "- Partie 3 : G√©n√©ration de la fiche technique\n",
    "\n",
    "## Outils utilis√©s\n",
    "- Python 3.14\n",
    "- pandas : manipulation de donn√©es\n",
    "- duckdb : requ√™tes SQL\n",
    "- plotly : visualisations\n",
    "- litellm : acc√®s √† Claude API\n",
    "- Claude Sonnet 4.5 : analyse assist√©e par IA\n",
    "\n",
    "## Auteur\n",
    "Hassan HOUSSEIN HOUMED\n",
    "\n",
    "## Licence\n",
    "Data : CC-BY\n",
    "Notebook : Libre d'utilisation\n",
    "\"\"\"\n",
    "\n",
    "# Sauvegarde du README\n",
    "with open('README.md', 'w', encoding='utf-8') as fichier:\n",
    "    fichier.write(readme_content)\n",
    "\n",
    "print(\"README.md cr√©√© avec succ√®s !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afb837c-1b29-4349-ba75-9df60a1e2b46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
